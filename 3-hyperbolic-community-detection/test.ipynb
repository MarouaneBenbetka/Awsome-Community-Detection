{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS ROG\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "INFO: Using numpy backend\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from utils.data_utils import data_loader , corpora, corpora_tools\n",
    "from utils.evaluation_utils import evaluation\n",
    "from utils.functions_utils.distribution_function import CategoricalDistributionSampler\n",
    "from utils.manifold.poincare_ball import PoincareBallExact\n",
    "from utils.optim_tools import rsgd\n",
    "from utils.embedding_utils.losses import graph_embedding_criterion, graph_community_criterion\n",
    "from utils.clustering_utils.poincare_em import PoincareEM\n",
    "from utils.clustering_utils.poincare_xmedoid import XMedoid\n",
    "from utils.clustering_utils.poincare_kmeans import PoincareKMeans\n",
    "from utils.clustering_utils.poincare_kmedoid import RiemannianKMedoids\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset football loaded\n",
      "Number of nodes: 115\n"
     ]
    }
   ],
   "source": [
    "dataset = 'football'\n",
    "\n",
    "X, Y = data_loader.load_corpus(dataset, directed=False)\n",
    "print(f\"Dataset {dataset} loaded\")\n",
    "print(f\"Number of nodes: {len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.nmi([y[0] for y in Y.values()],Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamethers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Wak\n",
    "PATH_LEN = 10\n",
    "PRECOMPUTE = 6\n",
    "CONTEXT_SIZE = 5\n",
    "\n",
    "# Embedding\n",
    "EMBEDDING_DIM = 8\n",
    "LEARNING_RATE = .01\n",
    "EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:00<00:00, 6576.65it/s]\n"
     ]
    }
   ],
   "source": [
    "X, Y = data_loader.load_corpus(dataset, directed=False)\n",
    "\n",
    "\n",
    "dataset_l1 = corpora.NeigbhorFlatCorpus(X, Y)\n",
    "\n",
    "dataset_l2 = corpora.RandomContextSizeFlat(X, Y, precompute=PRECOMPUTE,path_len=PATH_LEN, context_size=CONTEXT_SIZE)\n",
    "\n",
    "dataset_l3 = corpora_tools.from_indexable(torch.arange(0, len(X), 1).unsqueeze(-1))\n",
    "\n",
    "\n",
    "dataloader_l1 = DataLoader(dataset_l1,\n",
    "                           batch_size=20,\n",
    "                           shuffle=True,\n",
    "                           drop_last=False,\n",
    "                           )\n",
    "\n",
    "dataloader_l2 = DataLoader(dataset_l2, batch_size=20,\n",
    "                           shuffle=True, collate_fn=lambda tensor_list: (torch.cat(tensor_list, 0)[:,0] , torch.cat(tensor_list, 0)[:,1]))\n",
    "\n",
    "dataloader_l3 = DataLoader(dataset_l3,\n",
    "                           batch_size=20,\n",
    "                           shuffle=True,\n",
    "                           drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of communities :  12\n"
     ]
    }
   ],
   "source": [
    "frequency = dataset_l2.getFrequency()[:,1]\n",
    "# frequency = frequency**(3/4)\n",
    "frequency = frequency**(3/4)\n",
    "\n",
    "frequency /= frequency.sum()\n",
    "\n",
    "\n",
    "distribution = CategoricalDistributionSampler(frequency)\n",
    "n_community = len(set([communities[0] for key, communities in Y.items()]))\n",
    "print(\"Number of communities : \", n_community)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_embedding = nn.Embedding(len(X), EMBEDDING_DIM, max_norm=0.999)\n",
    "node_embedding.weight.data[:] = node_embedding.weight.data * 1e-2\n",
    "context_embedding = nn.Embedding(len(X), EMBEDDING_DIM, max_norm=0.999)\n",
    "context_embedding.weight.data[:] = context_embedding.weight.data * 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifold = PoincareBallExact\n",
    "optimizer_init = rsgd.RSGD(list(node_embedding.parameters()) + list(context_embedding.parameters()), LEARNING_RATE, manifold=manifold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:27<00:00,  2.77s/it]\n"
     ]
    }
   ],
   "source": [
    "def memory_transfer(x): return x\n",
    "\n",
    "\n",
    "for i in tqdm(range(EPOCHS)):\n",
    "    l2 = 0\n",
    "    for x, y in dataloader_l2:\n",
    "        optimizer_init.zero_grad()\n",
    "        pe_x = node_embedding(memory_transfer(x.long()))\n",
    "        pe_y = context_embedding(memory_transfer(y.long()))\n",
    "        ne = context_embedding(memory_transfer(\n",
    "            distribution.sample(sample_shape=(len(x), EMBEDDING_DIM)))).detach()\n",
    "        loss = graph_embedding_criterion(\n",
    "            pe_x, pe_y, z=ne, manifold=manifold).sum()\n",
    "        l2 += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_init.step()\n",
    "\n",
    "    l1 = 0.\n",
    "    \n",
    "    for x, y in dataloader_l1:\n",
    "        optimizer_init.zero_grad()\n",
    "        pe_x = memory_transfer(node_embedding(x.long()))\n",
    "        pe_y = memory_transfer(node_embedding(y.long()))\n",
    "        loss = graph_embedding_criterion(pe_x, pe_y, manifold=manifold).sum()\n",
    "        l1 += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_init.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = rsgd.RSGD(list(node_embedding.parameters()) +\n",
    "                      list(context_embedding.parameters()), LEARNING_RATE, manifold=manifold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:48<00:00,  4.90s/it]\n"
     ]
    }
   ],
   "source": [
    "def memory_transfer(x): return x\n",
    "\n",
    "\n",
    "for i in tqdm(range(EPOCHS)):\n",
    "\n",
    "    l1 = 0.\n",
    "    for x, y in dataloader_l1:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pe_x = node_embedding(memory_transfer(x.long()))\n",
    "        pe_y = node_embedding(memory_transfer(y.long()))\n",
    "\n",
    "        loss = graph_embedding_criterion(pe_x, pe_y, manifold=manifold).sum()\n",
    "        l1 += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    l2 = 0.\n",
    "    for x, y in dataloader_l2:\n",
    "        optimizer.zero_grad()\n",
    "        pe_x = memory_transfer(node_embedding(x.long()))\n",
    "        pe_y = memory_transfer(context_embedding(y.long()))\n",
    "        ne = context_embedding(memory_transfer(\n",
    "            distribution.sample(sample_shape=(len(x), 10)))).detach()\n",
    "        loss = graph_embedding_criterion(\n",
    "            pe_x, pe_y, z=ne, manifold=manifold).sum()\n",
    "        l2 += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    em_alg = PoincareEM(n_community)\n",
    "    em_alg.fit(memory_transfer(node_embedding.weight.data))\n",
    "\n",
    "    NF = em_alg.get_normalisation_coef()\n",
    "    pi, mu, sigma = em_alg.get_parameters()\n",
    "\n",
    "    pik = em_alg.get_pik(node_embedding.weight.data)\n",
    "\n",
    "    l3 = 0.\n",
    "\n",
    "    for x in dataloader_l3:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pe_x = node_embedding(memory_transfer(x[0].long()))\n",
    "        wik = pik[memory_transfer(x[0].long())]\n",
    "        loss = 1e-1 * graph_community_criterion(pe_x.squeeze(), wik.detach(\n",
    "        ), mu.detach(), sigma.detach(), NF.detach(), manifold=manifold).sum()\n",
    "        l3 += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "output_np = np.array(node_embedding.weight.data)\n",
    "output= node_embedding.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of communities (XMedoid): 115\n",
      "Real Number of communities: 12\n",
      "NMI Score (XMedoid): 0.8974118393814873\n"
     ]
    }
   ],
   "source": [
    "kmeans = PoincareKMeans(n_clusters=n_community, random_seed=0)\n",
    "\n",
    "centroids = kmeans.fit(output)\n",
    "predicted_clusters = kmeans.predict(output)\n",
    "\n",
    "nmi_score_kmedoids = evaluation.nmi(predicted_clusters, Y)\n",
    "\n",
    "print(\"Number of communities (XMedoid):\", len(set(predicted_clusters)))\n",
    "print(\"Real Number of communities:\", n_community)\n",
    "print(\"NMI Score (XMedoid):\", nmi_score_kmedoids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
